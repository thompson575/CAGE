---
title: "Feature Extraction and Selection with Microarray Data"
author: "John Thompson"
date: "2023-02-23"
output: 
  powerpoint_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set( echo=FALSE, message=FALSE, 
                       warning=FALSE, fig.align='center')
```

```{r }
library(tidyverse)

home <- "C:/Projects/RCourse/Masterclass/CAGE/"

source(file.path(home, "code/display_functions.R"))

patientDF  <- readRDS( file.path(home, "data/cache/subjects.rds") )

r <- readRDS(file.path(home, "data/cache/aegis1_loss.rds"))
```

## Extraction and Selection

* **Feature Extraction** derives transformed values (features) that capture the majority of the information in a dataset. 

<br>

* Feature extraction is sometimes called **dimensionality reduction**.  

<br>

* **Feature Selection** identifies a subset of the features important for a particular task.


## Principal Component Analysis (PCA)

* Method for dimensionality reduction (feature extraction)

* uncorrelated linear combinations of the raw data  

* successively orientated in the direction of maximum variance  

![](figs/pca.png){height=75%, width=75%}

## AEGIS Trials I and II

Airway Epithelium Gene Expression in the Diagnosis of Lung Cancer

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
patientDF %>%
  ggplot( aes( x= study, fill = diagnosis)) +
  geom_bar( position = "dodge") +
  labs( title = "Numbers of cancer and benign cases in each study")
```

## The Investigation

- Develop a model to classify cancer/benign

- Use AEGIS-1 to train the model and AEGIS-2 to validate  

- Methods Investigated

  - No Feature Extraction .. Select the M probes that best classify  
  - Use PCA for feature extraction .. Use the first M PCs
  - PCA for feature extraction .. Select the M PCs that best classify  
  
- PCA applied to the raw data and to the scaled raw data  

## Loss Function

Logistic regression models are to predict the cancer cases

Model performance is assessed using the cross-entropy loss
\[
- \frac{1}{N} \sum_{i=1}^N y_i log(\hat{y}_i) + (1-y_i) log(1-\hat{y}_i)
\]

This is equivalent to the log-likelihood for a logistic regression

## Results: In-sample 1-50 Features

```{r}
list( probe  = r$probeSelectedDF,
      vPCsel = r$vpcaSelectedDF,
      vPCord = r$vpcaOrderedDF,
      cPCsel = r$cpcaSelectedDF,
      cPCord = r$cpcaOrderedDF)  %>%
  plot_method_comparison(inloss) +
  ggtitle("In-sample loss for the 5 feature selection methods")
```

## Results: In-sample 1-8 Features

```{r}
list( probe  = r$probeSelectedDF %>% slice(1:8),
      vPCsel = r$vpcaSelectedDF  %>% slice(1:8),
      vPCord = r$vpcaOrderedDF   %>% slice(1:8),
      cPCsel = r$cpcaSelectedDF  %>% slice(1:8),
      cPCord = r$cpcaOrderedDF   %>% slice(1:8))  %>%
  plot_method_comparison(inloss) +
  ggtitle("In-sample loss for the 5 feature selection methods")

```

## Results: Out-of-sample 1-50 Features

```{r}
list( probe  = r$probeSelectedDF,
      vPCsel = r$vpcaSelectedDF,
      vPCord = r$vpcaOrderedDF,
      cPCsel = r$cpcaSelectedDF,
      cPCord = r$cpcaOrderedDF)  %>%
  plot_method_comparison(outloss) +
  ggtitle("Out-of-sample loss for the 5 feature selection methods")
```


## Results: Out-of-sample 1-8 Features

```{r}
list( probe  = r$probeSelectedDF %>% slice(1:8),
      vPCsel = r$vpcaSelectedDF  %>% slice(1:8),
      vPCord = r$vpcaOrderedDF   %>% slice(1:8),
      cPCsel = r$cpcaSelectedDF  %>% slice(1:8),
      cPCord = r$cpcaOrderedDF   %>% slice(1:8))  %>%
  plot_method_comparison(outloss) +
  ggtitle("Out-of-sample loss for the 5 feature selection methods")
```

## Conclusions

* Features selected in AEGIS-1 perform poorly in AEGIS-2  
* Scaled and unscaled PCA performs equally well  
* In-sample performance suggests that 
  - selected PCs out perform selected probes  
  - the first PCs predict particularly poorly  
* Out-of-sample results do not support the use of selected PCs  
  - suggesting that small PCs over-fit the training data