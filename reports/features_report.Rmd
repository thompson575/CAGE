---
title: 'CAGE: Feature Selection'
author: "John Thompson"
date: "21st March 2023"
output: 
    prettydoc::html_pretty:
      theme: hpstr
---

```{r setup, include=FALSE}
# --- theme hpstr requires  R package prettydoc
library(tidyverse)
library(fs)
library(ggthemes)

# --- source display functions
code <- "C:/Projects/RCourse/Masterclass/CAGE/code"
source(path(code, "display_functions.R") )

# --- read data
cache <- "C:/Projects/RCourse/Masterclass/CAGE/data/cache"
loss <- readRDS(path(cache, "feature_loss.rds"))

# --- chunk options
knitr::opts_chunk$set(echo = FALSE, fig.align = "center",
                      message=FALSE)

# --- plotting theme from ggthemes
theme_set(theme_economist())
```

## CAGE

CAGE (Classification After Gene Expression) explores ways of selecting features from a gene expression study for use in patient classification. In particular, it investigates the pros and cons of basing the classification on the principal components of the gene expression.

## Feature Selection Methods

Using the random sample of 1000 probes five methods of feature selection were compared  

* sets of the M most predictive probes in univariate logistic regression  
* sets of the M most predictive unscaled PCs in univariate logistic regression  
* sets of the first M unscaled PCs  
* sets of the M most predictive scaled PCs in univariate logistic regression  
* sets of the first M scaled PCs  

where M ranged from 1 to 50.

A scaled PCA uses the principal components of the correlations and an unscaled PCA uses the principal components of the covariances.

Selections were made using the training data that consisted of the 1000 probes measured on the subjects from AEGIS-1 and validation was based on the same probes measured on the subjects from AEGIS-2.

The Cross-Entropy loss is used to compare models. It takes the form,
\[
- \frac{1}{N} \sum_{i=1}^N y_i log(\hat{y}_i) + (1-y_i) log(1-\hat{y}_i)
\]
where N is the number of subjects, $y_i$ is 0 for benign cases and 1 for cancer cases and $\hat{y}_i$ is the predicted probability of the case being cancer under whatever model is being used.

The training data are used to define the principal component, make the selections and fit the logistic regression model. That final model is then used with the validation data.

## M most Predictive Probes

Figure 1 shows the loss when sets of M probes are used to predict cancer in a multivariate logistic regression. The in-sample loss declines steadily as more probes are used, but the out-of-sample loss increases dramatically after a 4 probe model.

```{r}
plot_in_out_loss(loss$probeSelectedDF) +
  labs( title = "Fig 1: Loss from M Selected Probes",
        y     = "Loss",
        x     = "Number of Probes (M)")
```

The first 4 selected probes are show below. Here the loss refers to the loss in a univarate logistic regression using only that probe.

```{r}
loss$probeUniDF %>%
  arrange( loss ) %>%
  slice(1:4) %>%
  rename( probe = x) %>%
  print()
```

Manual searching of the Ensembl database at https://www.ensembl.org/Homo_sapiens/Info/Index showed that  

* ENSG00000124357 maps to the NAGK gene (N-acetylglucosamine kinase)  
* ENSG00000118322 maps to the ATP10B gene (ATPase phospholipid transporting 10B)  
* ENSG00000140464 maps to the PML gene (PML nuclear body scaffold)  
* ENSG00000026950 maps to the BTN3A1 gene (Butyrophilin subfamily 3 member A1)  

## M most Predictive Unscaled PCs

Figure 2 shows the loss when sets of M selected principal components are used to predict cancer in a multivariate logistic regression. Although the in-sample loss decreases steadily the out-of-sample loss is always greater and it increases as more PCs are added. This seems to reflect the instability of the principal component analysis.

```{r}
plot_in_out_loss(loss$vpcaSelectedDF) +
  labs( title = "Fig 2: Loss from Selected Unscaled PCs",
        y     = "Loss",
        x     = "Number of PCs (M)")
```

## First M Unscaled PCs

Figure 3 shows the loss when the first M principal components (those with the largest eigenvalues) are used as the predictors. Once again the in-sample loss decreases steadily. The out-of-sample loss is higher and remains flat or perhaps drifts down for the first 6 PCs before rising dramatically. This would be consistent with the first 6 PCs being relatively well-defined in the training data, so that they still meaning in the validation sample.

```{r}
plot_in_out_loss(loss$vpcaOrderedDF) +
  labs( title = "Fig 3: Loss from Orderedered Unscaled PCs",
        y     = "Loss",
        x     = "Number of PCs (M)") 
```

## M most Predictive Scaled PCs

Figure 4 shows the loss when sets of M selected scaled principal components are used to predict cancer in a multivariable logistic regression. Although the in-sample loss decreases steadily the out-of-sample loss is always greater and it increases as more PCs are added. This seems to reflect the instability of the principal component analysis.

```{r}
plot_in_out_loss(loss$cpcaSelectedDF) +
  labs( title = "Fig 4: Loss from Selected Scaled PCs",
        y     = "Loss",
        x     = "Number of PCs (M)")
```

## First M Scaled PCs

Figure 5 shows the loss when the first M scaled principal components (those with the largest eigenvalues) are used as the predictors. Once again the in-sample loss decreases steadily. The out-of-sample loss is higher and remains flat or perhaps drifts down for the first 6 PCs before rising dramatically. This would be consistent with the first 6 PCs being relatively well-defined in the training data, so that they still meaning in the validation sample.

```{r}
plot_in_out_loss(loss$cpcaOrderedDF) +
  labs( title = "Fig 5: Loss from Orderedered Scaled PCs",
        y     = "Loss",
        x     = "Number of PCs (M)")
```

## Comparison of Methods

Figure 6 shows the in-sample losses of the five methods of feature selection. Based on the in-sample performance alone it appears that selected PCs give the smallest loss.

```{r}
list( PBsel  = loss$probeSelectedDF,
      vPCsel = loss$vpcaSelectedDF,
      vPCord = loss$vpcaOrderedDF,
      cPCsel = loss$cpcaSelectedDF,
      cPCord = loss$cpcaOrderedDF
) %>%
  plot_method_comparison(inloss) +
    ggtitle("Fig 6: In-sample loss for the 5 feature selection methods")
```

Figure 6a shows the same plot concentrated on 8 or fewer features.

```{r}
list( PBsel  = loss$probeSelectedDF     %>% slice(1:8),
      vPCsel = loss$vpcaSelectedDF      %>% slice(1:8),
      vPCord = loss$vpcaOrderedDF       %>% slice(1:8),
      cPCsel = loss$cpcaSelectedDF      %>% slice(1:8),
      cPCord = loss$cpcaOrderedDF       %>% slice(1:8)
) %>%
  plot_method_comparison(inloss) +
    ggtitle("Fig 6a: In-sample loss for the 5 feature selection methods")
```

Figure 7 shows the out-of-sample performance of the 5 feature selection methods. None of the methods performs well with a large number of features, but selected PCs performs particularly poorly.

```{r}
list( PBsel  = loss$probeSelectedDF,
      vPCsel = loss$vpcaSelectedDF,
      vPCord = loss$vpcaOrderedDF,
      cPCsel = loss$cpcaSelectedDF,
      cPCord = loss$cpcaOrderedDF
) %>%
  plot_method_comparison(outloss) +
    ggtitle("Fig 7: Out-of-sample loss for the 5 feature selection methods")
```

Figure 7a shows the same data as figure 7 but concentrates to 8 or fewer features. Probe selection improves the loss up to 4 features. Using the initial PCs improves the loss slowly for the first 6 features, but is never as good as the top 4 probes and selected PCs performs particularly poorly.

```{r}
list( PBsel  = loss$probeSelectedDF     %>% slice(1:8),
      vPCsel = loss$vpcaSelectedDF      %>% slice(1:8),
      vPCord = loss$vpcaOrderedDF       %>% slice(1:8),
      cPCsel = loss$cpcaSelectedDF      %>% slice(1:8),
      cPCord = loss$cpcaOrderedDF       %>% slice(1:8)
) %>%
  plot_method_comparison(outloss) +
    ggtitle("Fig 7a: Out-of-sample loss for the 5 feature selection methods")
```


## Conclusions

The instability of the principal components with small eigenvalues means that feature selection based on them performs very poorly. The first few principal components are more stable but they do not perform as well as selected probes.  

The results for scaled and unscaled PCs are very similar.

## Issues

* It is possible that the full set of over 21,000 probes will contain probes that are much more predictive. Stronger effects might make the analysis more stable.  

* The relatively small number of patients in AEGIS-1 (n=375) contributes to instability of the principal component analysis. The conclusions might be different for studies in which n is larger.  

* Perhaps it would make sense to restrict PC selection to M from the top R e.g. the best 5 out of the first 20 PCs  
