---
title: 'CAGE: Feature Selection'
author: "John Thompson"
date: "2023-02-17"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)

home <- "C:/Projects/RCourse/Masterclass/CAGE"

# ---------------------------------------------
# Named list of results containing
# pcUniDF, pcSelDF, pcOrdDF, pbUniDF, pbSelDF
#
  
readRDS( file.path(home, "data/dataStore/classification_loss.rds")) -> results

knitr::opts_chunk$set(echo = FALSE, fig.align = "center",
                      message=FALSE)
```

## CAGE

CAGE (Classification After Gene Expression) explores ways of selecting features from a gene expression study for patient classification. In particular, it investigates the pros and cons of basing the classification on the principal components of the gene expression.

## Feature Selection Methods

Using the random sample of 1000 probes three methods of feature selection were compared  

* sets of the M most predictive probes in univariate logistic regression  
* sets of the M most predictive PCs in univariate logistic regression  
* sets of the first M PCs  

where M ranged from 1 to 50.

Selections were made using the training data that consisted of the 1000 probes measured on the subjects from AEGIS-1 and validation was based on the same probes measured on the subjects from AEGIS-2.

The Cross-Entropy loss is used to compare models. It takes the form,
\[
- \frac{1}{N} \sum_{i=1}^N y_i log(\hat{y}_i) + (1-y_i) log(1-\hat{y}_i)
\]
where N is the number of subjects, $y_i$ is 0 for benign cases and 1 for cancer cases and $\hat{y}_i$ is the predicted probability of the case being cancer under whatever model is being used.

The training data are used to define the principal component, make the selections and fit the logistic regression model. That final model is then used with the validation data.

## M most Predictive Probes

Figure 1 shows the loss when sets of M probes are used to predict cancer in a multivariable logistic regression. The in-sample loss declines steadily as more probes are used, but the out-of-sample loss increases dramatically after a 4 probe model.

```{r}
results$pbSelDF %>%
  ggplot( aes(x = n, y = inloss)) +
  geom_line( size=1.2, colour="blue") +
  geom_line( aes(y = outloss), size=1.2, colour="red") +
  labs( title = "Fig 1: Loss from M Selected Probes",
        y     = "Loss",
        x     = "Number of Probes (M)") +
  geom_text( x=35, y=0.58, label="In-sample", colour="blue") +
  geom_text( x=35, y=0.90, label="Out-of-sample", colour="red") 
```

The first 4 selected probes are show below. Here the loss refers to the loss in a univarable logistic regression using only that probe.

```{r}
results$pbUniDF %>%
  arrange( loss ) %>%
  slice(1:4) %>%
  rename( probe = x) %>%
  print()
```

Manual searching of the Ensembl database at https://www.ensembl.org/Homo_sapiens/Info/Index showed that  

* ENSG00000124357 maps to the NAGK gene (N-acetylglucosamine kinase)  
* ENSG00000118322 maps to the ATP10B gene (ATPase phospholipid transporting 10B)  
* ENSG00000140464 maps to the PML gene (PML nuclear body scaffold)  
* ENSG00000026950 maps to the BTN3A1 gene (Butyrophilin subfamily 3 member A1)  

## M most Predictive PCs

Figure 2 shows the loss when sets of M selected principal components are used to predict cancer in a multivariable logistic regression. Although the in-sample loss decreases steadily the out-of-sample loss is always greater and it increases as more PCs are added. This seems to reflect the instability of the principal component analysis.

```{r}
results$pcSelDF %>%
  ggplot( aes(x = n, y = inloss)) +
  geom_line( size=1.2, colour="blue") +
  geom_line( aes(y = outloss), size=1.2, colour="red") +
  labs( title = "Fig 2: Loss from Selected PCs",
        y     = "Loss",
        x     = "Number of PCs") +
  geom_text( x=35, y=0.58, label="In-sample", colour="blue") +
  geom_text( x=35, y=2.00, label="Out-of-sample", colour="red")
```

## First M PCs

Figure 3 shows the loss when the first M principal components (those with the largest eigenvalues) are used as the predictors. Once again the in-sample loss decreases steadily. The out-of-sample loss is higher and remains flat or perhaps drifts down for the first 6 PCs before rising dramatically. This would be consistent with the first 6 PCs being relatively well-defined in the training data, so that they still meaning in the validation sample.

```{r}
results$pcOrdDF %>%
  ggplot( aes(x = n, y = inloss)) +
  geom_line( size=1.2, colour="blue") +
  geom_line( aes(y = outloss), size=1.2, colour="red") +
  labs( title = "Fig 3: Loss from Ordered PCs",
        y     = "Loss",
        x     = "Number of PCs") +
  geom_text( x=35, y=0.58, label="In-sample", colour="blue") +
  geom_text( x=35, y=1.00, label="Out-of-sample", colour="red")
```

## Comparison of Methods

Figure 4 shows the in-sample losses of the three methods of feature selection. Based on the in-sample performance alone it appears that selected PCs give the smallest loss.

```{r}
results$pcSelDF %>%
  rename( pcSel = inloss) %>%
  left_join( results$pbSelDF %>% rename( probe  = inloss), by = "n") %>%
  left_join( results$pcOrdDF %>% rename( pcOrd = inloss), by = "n") %>%
  pivot_longer(c("probe", "pcSel", "pcOrd"), names_to = "method", values_to = "loss") %>%
  mutate( source = str_replace(method, "loss", ""))  %>%
  ggplot( aes(x = n, y = loss, colour=method)) +
  geom_line( size=1.2) +
  labs( title = "Fig 4: In-sample loss for the 3 feature selection methods",
        y     = "Loss",
        x     = "Number of predictors") +
  theme( legend.position = c(0.15, 0.25))
```

Figure 5 shows the out-of-sample performance of the 3 feature selection methods. None of the methods performs well with a large number of features, but selected PCs performs particularly poorly.

```{r}
results$pcSelDF %>%
  rename( pcSel = outloss) %>%
  left_join( results$pbSelDF %>% rename( probe = outloss), by = "n") %>%
  left_join( results$pcOrdDF %>% rename( pcOrd = outloss), by = "n") %>%
  pivot_longer(c("probe", "pcSel", "pcOrd"), names_to = "method", values_to = "loss") %>%
  mutate( source = str_replace(method, "loss", ""))  %>%
  ggplot( aes(x = n, y = loss, colour=source)) +
  geom_line( size=1.2) +
  labs( title = "Fig 5: Out-of-sample loss for the 3 feature selection methods",
        y     = "Loss",
        x     = "Number of predictors") +
  theme( legend.position = c(0.15, 0.85))
```

Figure 6 shows the same data as figure 5 but concentrates to 6 or fewer features. Probe selection improves the loss up to 4 features. Using the initial PCs improves the loss slowly for the first 6 features, but is never as good as the top 4 probes and selected PCs performs particularly poorly.

```{r}
results$pcSelDF %>%
  rename( pcSel = outloss) %>%
  left_join( results$pbSelDF %>% rename( probe = outloss), by = "n") %>%
  left_join( results$pcOrdDF %>% rename( pcOrd = outloss), by = "n") %>%
  pivot_longer(c("probe", "pcSel", "pcOrd"), names_to = "method", values_to = "loss") %>%
  mutate( source = str_replace(method, "loss", ""))  %>%
  filter( n <= 6 ) %>%
  ggplot( aes(x = n, y = loss, colour=source)) +
  geom_line( size=1.2) +
  labs( title = "Fig 6: Out-of-sample loss for the 3 feature selection methods",
        y     = "Loss",
        x     = "Number of predictors") +
  theme( legend.position = c(0.15, 0.85))
```

## Conclusions

The instability of the principal components with small eigenvalues means that feature selection based on them performs very poorly. The first few principal components are more stable but they do not perform as well as selected probes.

## Issues

* It is possible that the full set of over 21,000 probes will contain probes that are much more predictive. Stronger effects might make the analysis more stable.  
* The relatively small number of patients in AEGIS-1 (n=375) contributes to instability of the principal component analysis. The conclusions might be different for studies in which n is larger.  
* Perhaps it would make sense to restrict PC selection to M from the top R e.g. the best 5 out of the first 20 PCs  
